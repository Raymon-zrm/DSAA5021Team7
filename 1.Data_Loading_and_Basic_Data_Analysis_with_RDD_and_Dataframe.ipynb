{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "389fb6d9",
   "metadata": {},
   "source": [
    "# MovieLens: Spark-based Big Data Recommendation Analysis\n",
    "\n",
    "# Task1-Data Loading and Basic Data Analysis with RDD and Dataframe\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878c96f8",
   "metadata": {},
   "source": [
    "# 1. Data Structure Operation Test\n",
    "\n",
    "## 1.1. Dataframe Operation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaaa3f4f-c680-40dd-b307-380d9a3b5b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/09 11:23:20 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|   John| 28|\n",
      "|  Linda| 33|\n",
      "|Michael| 22|\n",
      "+-------+---+\n",
      "\n",
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "| John| 28|\n",
      "|Linda| 33|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# create a simple DataFrame, store into a partition directory\n",
    "data = [(\"John\", 28), (\"Linda\", 33), (\"Michael\", 22)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Filter the DataFrame based on age > 25\n",
    "df_filtered = df.filter(df.Age > 25)\n",
    "\n",
    "# Show the DataFrame\n",
    "df_filtered.show()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0640101c-92dc-4ca6-a987-d6482bd24f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/01 13:13:31 INFO SparkEnv: Registering MapOutputTracker\n",
      "23/12/01 13:13:31 INFO SparkEnv: Registering BlockManagerMaster\n",
      "23/12/01 13:13:31 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/12/01 13:13:31 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered and aggregated RDD result: 104\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"rdd_example\").getOrCreate()\n",
    "\n",
    "# Get the SparkContext from SparkSession\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Create a list of integers\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Use map function to double each value in the RDD\n",
    "mapped_rdd = rdd.map(lambda x: x * 2)\n",
    "\n",
    "# Use filter function to get values greater than 5 in the RDD\n",
    "filtered_rdd = mapped_rdd.filter(lambda x: x > 5)\n",
    "\n",
    "# Sum the values in the filtered RDD\n",
    "sum_result = filtered_rdd.reduce(lambda x, y: x + y)\n",
    "\n",
    "# Print the sum\n",
    "print(\"Filtered and aggregated RDD result:\", sum_result)\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc824cb",
   "metadata": {},
   "source": [
    "# 2. Data Load with Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b96d3d3-b943-4689-86c4-5233a333646b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_file = \"file:///mnt/gwx/datasets/ml-25m/ratings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d89d4f8-0feb-4552-9f4d-6489fb988e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd010545-81ab-4738-9585-67f1113a87ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/01 14:19:17 INFO SparkEnv: Registering MapOutputTracker\n",
      "23/12/01 14:19:17 INFO SparkEnv: Registering BlockManagerMaster\n",
      "23/12/01 14:19:17 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/12/01 14:19:17 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ratings_file = \"gs://dataproc-staging-asia-southeast2-933547737015-zijhgarf/ratings.csv\"\n",
    "spark = SparkSession.builder.appName(\"Rating Analysis\").getOrCreate()\n",
    "ratings_raw_data = spark.sparkContext.textFile(ratings_file)\n",
    "ratings_raw_data_header = ratings_raw_data.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a85e6943-f6af-466a-8ffc-c2e332614b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['userId,movieId,rating,timestamp',\n",
       " '1,296,5.0,1147880044',\n",
       " '1,306,3.5,1147868817']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_raw_data.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61834519-2c92-4208-87a5-ed6ad0eb8643",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_data = ratings_raw_data.filter(lambda line: line!=ratings_raw_data_header)\\\n",
    "    .map(lambda line: line.split(\",\")).map(lambda tokens: (tokens[0],tokens[1],tokens[2])).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9da0759-b26c-40b0-8b9d-28b5fa77c104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('1', '296', '5.0'), ('1', '306', '3.5'), ('1', '307', '5.0')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_data.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc938e4",
   "metadata": {},
   "source": [
    "# 3. Data Analysis\n",
    "\n",
    "## 3.1. Statistic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4de179a9-c4e0-48ea-a553-d7e7e741a7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:===============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评分数量：25000095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "num_ratings = ratings_data.count()\n",
    "print(f\"Num of ratings:{num_ratings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54db062d-8d49-48d5-873b-09435ba52370",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b71cd617-3e74-4550-ab80-70b1aaa9c5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/01 14:33:00 INFO SparkEnv: Registering MapOutputTracker\n",
      "23/12/01 14:33:00 INFO SparkEnv: Registering BlockManagerMaster\n",
      "23/12/01 14:33:00 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/12/01 14:33:00 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "[Stage 2:=================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|movieId|       avg(rating)|\n",
      "+-------+------------------+\n",
      "|   1580|3.5817083457378187|\n",
      "|   1959| 3.634788998530338|\n",
      "|   2366| 3.543409877319912|\n",
      "|   5300| 3.626334519572954|\n",
      "|   6620|3.7897800776196635|\n",
      "|   1591| 2.637077181835171|\n",
      "|   3175|3.6077836141619484|\n",
      "|  96488|3.9735049205147615|\n",
      "| 175197| 2.754918032786885|\n",
      "|  44022|3.2593627146699773|\n",
      "| 160563|3.0350500715307582|\n",
      "|    471|3.6579813752234034|\n",
      "|   1645| 3.547347362181387|\n",
      "|   3918|2.9821298322392416|\n",
      "|   7982|3.6235955056179776|\n",
      "|   8638|3.9717508278145695|\n",
      "|   1088|  3.25002094679514|\n",
      "|   6357| 3.669491525423729|\n",
      "|   3997|2.0634660421545665|\n",
      "|   4519|3.3481739844070577|\n",
      "+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Rating Analysis\").getOrCreate()\n",
    "\n",
    "# Read the ratings data into a Spark DataFrame\n",
    "ratings_df = spark.read.csv(\"gs://dataproc-staging-asia-southeast2-933547737015-zijhgarf/ratings.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Process the data by Spark DataFrame\n",
    "ratings_df.groupBy(\"movieId\").avg(\"rating\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d83a2234-53b1-47b6-9aa6-e3b862e216f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|movieId|average_rating|\n",
      "+-------+--------------+\n",
      "| 207095|           5.0|\n",
      "| 202181|           5.0|\n",
      "| 122193|           5.0|\n",
      "| 137018|           5.0|\n",
      "| 131628|           5.0|\n",
      "+-------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "top_movies = (ratings_df.groupBy(\"movieId\")\n",
    "              .avg(\"rating\")\n",
    "              .withColumnRenamed(\"avg(rating)\", \"average_rating\")\n",
    "              .orderBy(\"average_rating\", ascending=False)\n",
    "              .limit(5))\n",
    "\n",
    "# Show results\n",
    "top_movies.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52b34aaa-133e-46c9-a691-8c8c25338425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
      "|      5|Father of the Bri...|              Comedy|\n",
      "|      6|         Heat (1995)|Action|Crime|Thri...|\n",
      "|      7|      Sabrina (1995)|      Comedy|Romance|\n",
      "|      8| Tom and Huck (1995)|  Adventure|Children|\n",
      "|      9| Sudden Death (1995)|              Action|\n",
      "|     10|    GoldenEye (1995)|Action|Adventure|...|\n",
      "|     11|American Presiden...|Comedy|Drama|Romance|\n",
      "|     12|Dracula: Dead and...|       Comedy|Horror|\n",
      "|     13|        Balto (1995)|Adventure|Animati...|\n",
      "|     14|        Nixon (1995)|               Drama|\n",
      "|     15|Cutthroat Island ...|Action|Adventure|...|\n",
      "|     16|       Casino (1995)|         Crime|Drama|\n",
      "|     17|Sense and Sensibi...|       Drama|Romance|\n",
      "|     18|   Four Rooms (1995)|              Comedy|\n",
      "|     19|Ace Ventura: When...|              Comedy|\n",
      "|     20|  Money Train (1995)|Action|Comedy|Cri...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the movies data into a Spark DataFrame\n",
    "movies_df = spark.read.csv(\"gs://dataproc-staging-asia-southeast2-933547737015-zijhgarf/movies.csv\", header=True, inferSchema=True)\n",
    "\n",
    "movies_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "757f96b1-590f-40b6-bd91-f5415ba1adf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "| 122193|   Kit Carson (1940)|     Romance|Western|\n",
      "| 131628|       Loaded (2014)|        Comedy|Drama|\n",
      "| 137018|A Sister's Reveng...|Drama|Mystery|Thr...|\n",
      "| 202181| Warlock Moon (1973)|     Horror|Thriller|\n",
      "| 207095|Windy City Heat (...|  Comedy|Documentary|\n",
      "+-------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "specific_movieIds = [207095, 202181, 122193, 137018, 131628]\n",
    "\n",
    "# Process data by filter and isin\n",
    "specific_movies_df = movies_df.filter(movies_df.movieId.isin(specific_movieIds))\n",
    "\n",
    "# Show the result\n",
    "specific_movies_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9397abf5",
   "metadata": {},
   "source": [
    "## 3.2. Simple Recommendation with Bayesian Average Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db318907-3961-4992-9ce5-1563f0558250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------------------+-----------------+--------------------+\n",
      "|movieId|count|              mean|     bayesian_avg|               title|\n",
      "+-------+-----+------------------+-----------------+--------------------+\n",
      "|    318|81482| 4.413576004516335|4.406637765911728|Shawshank Redempt...|\n",
      "|    858|52498| 4.324336165187245|4.314311946380134|Godfather, The (1...|\n",
      "|     50|55366| 4.284353213163313|4.275147751556197|Usual Suspects, T...|\n",
      "|   1221|34188|4.2617585117585115|4.247196813161273|Godfather: Part I...|\n",
      "|    527|60411| 4.247579083279535|4.239392970515866|Schindler's List ...|\n",
      "+-------+-----+------------------+-----------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# 1. Compute every movie's rating count and average rating\n",
    "movie_stats = ratings_df.groupBy(\"movieId\").agg(\n",
    "    F.count(\"rating\").alias(\"count\"),\n",
    "    F.mean(\"rating\").alias(\"mean\")\n",
    ")\n",
    "\n",
    "# 2. Compute the global average rating\n",
    "C = movie_stats.select(F.mean(\"count\")).collect()[0][0]\n",
    "m = movie_stats.select(F.mean(\"mean\")).collect()[0][0]\n",
    "\n",
    "# 3. Define the Bayesian average rating function\n",
    "def bayesian_avg(count, mean):\n",
    "    return (C * m + count * mean) / (C + count)\n",
    "\n",
    "bayesian_avg_udf = F.udf(bayesian_avg)\n",
    "\n",
    "bayesian_avg_ratings = movie_stats.withColumn(\n",
    "    \"bayesian_avg\", bayesian_avg_udf(\"count\", \"mean\")\n",
    ")\n",
    "\n",
    "# 4. Combine the movies DataFrame\n",
    "movie_stats = bayesian_avg_ratings.join(\n",
    "    movies_df.select(\"movieId\", \"title\"), \"movieId\"\n",
    ")\n",
    "\n",
    "# 5. Sort the movies by their Bayesian average rating and show the top 5\n",
    "top_movies = movie_stats.orderBy(F.desc(\"bayesian_avg\")).limit(5)\n",
    "top_movies.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8_torch2.1",
   "language": "python",
   "name": "python3.8_torch2.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
